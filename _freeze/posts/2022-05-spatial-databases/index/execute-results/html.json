{
  "hash": "e9cc2b035c9aa444b5082de6a159d96e",
  "result": {
    "markdown": "---\ntitle: \"Storing spatial data in databases\"\ndescription: |\n  Using PostGIS and never worrying about how and where to store your spatial data.\nauthor: \"Ivan Dubrovin\"\ndate: \"2022-05-07\"\ncategories: [PostGIS]\nimage: image.png\n---\n\n\nI believe that removing unnecessary choices from the workflow helps to focus on the important parts.\nFrameworks that enforce rules boost productivity.\nIf you always organize an R project as a package or as a `targets` pipeline, you don't need to think about directory structure.\nIf you always use `Black` and `Flake8` to automatically format Python code, you don't need to keep the PEP8 conventions in mind.\nInstead, you spend the most time possible on the content.\n\nI see the use of databases for storing spatial data as a similar form of workflow optimization.\nIf you always use databases, you have one less thing to worry about when initializing a new project.\nIn this post I want to describe how I set up my spatial databases for the projects I work on.\n\n# Why use databases?\n\nFor people who work with large amount of data daily this question might seem silly.\nBut it is not that obvious.\nI couldn't answer the question without thinking about it for a couple of minutes first, so it is probably worthwhile to list a couple of reasons.\n\n- Everything is stored in one place, organized in the same predictable way.\nYou don't have to think where on the file system to put the data, how to name the files and folders, how to specify paths to open the data.\n- Lots of operations can be performed in the database which minimizes the data transfer.\nThis is important when you need to be memory efficient and when the data is transferred through a network.\n- Databases are easy to back up and transfer to different machines.\nThere are established and well-documented tools to perform all kinds of maintenance with databases.\n- It is a workflow optimization if you commit to use them in all projects.\n\n# Setting up a spatial database\n\nThere are multiple spatial database frameworks. \nAs far as I know, the most popular one is the PostGIS extension for PostgreSQL databases.\nThat's the one I use.\nLots of useful information can be found on the [PostGIS website](https://postgis.net/), including the installation instructions and documentation.\nI work on MacOS, where the easiest way to install it is thorough `Homebrew`.\n\nThe first step is to create the database and connect to it.\n\n```sh\ncreatedb projects\npsql projects\n```\n\nA good practice is to separate the extensions into their own schema, so that the workspace is more clean.\nThe collection of commands below creates a new schema to store the extensions, activates the extensions for vector and raster spatial data, and modifies the search path so that the functions from the extensions are findable by PostgreSQL.\n\n```sql\nCREATE SCHEMA extensions;\nCREATE EXTENSION postgis SCHEMA extensions;\nCREATE EXTENSION postgis_raster SCHEMA extensions;\nALTER DATABASE projects SET search_path = \"$user\", public, extensions;\n```\n\nAltered this way, the search path will be updated only on the next reconnect to the database.\nTo take advantage of the search path, either reconnect or modify it for the current session with a following command:\n\n```sql\nSET search_path = \"$user\", public, extensions;\n```\n\nTo make sure that the extensions are installed correctly and the functions are findable, check the version:\n\n```sql\nSELECT PostGIS_Version();\n```\n\nFor me the results looks like this:\n\n```\n            postgis_version            \n---------------------------------------\n 3.2 USE_GEOS=1 USE_PROJ=1 USE_STATS=1\n```\n\nWe now have a database ready to store spatial data.\nNice!\nThe next sections describe tools that come with PostGIS to load spatial data into databases.\n\n# Uploading vector data\n\nSome vector data can be stored in a database even without extensions.\nFor points, for example, the easiest way is to store coordinates as separate columns.\nHowever, other simple features (lines, polygons, and their collections) are not as easy to store in plain databases.\nMoreover, storing coordinates as simple numbers does not allow any special treatment that spatial data usually needs, such as spatial subsetting, intersection detection, and so on.\nThat's where PostGIS comes in.\n\n## Example dataset\n\nAs an example vector dataset I will use a subset of data I collected for one of my current projects.\nIt is a grid of points over Moscow with travel times to the center of the city reported by Yandex.Maps for different travel modes.\nEach point has four travel times and a timestamp of the data collection.\nHere is how the table looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(here)\n\nshp <- here(\"posts\", \"2022-05-spatial-databases\", \"shp\")\nmoscow_shp <- file.path(shp, \"moscow_raw_subset.shp\")\nmoscow <- read_sf(moscow_shp)\nknitr::kable(head(moscow, n = 5))\n```\n\n::: {.cell-output-display}\n|PUBLIC  |CAR  |WALK     |CYCLE   |DATE                       |geometry                  |\n|:-------|:----|:--------|:-------|:--------------------------|:-------------------------|\n|1 h 2 m |42 m |2 h 16 m |1 h 6 m |2022-04-21T16:30:17.093661 |POINT (37.53524 55.813)   |\n|1 h     |42 m |2 h      |58 m    |2022-04-21T18:02:14.752148 |POINT (37.54322 55.8131)  |\n|1 h 1 m |44 m |1 h 59 m |57 m    |2022-04-21T16:30:51.351708 |POINT (37.55119 55.81319) |\n|54 m    |41 m |2 h 13 m |1 h 4 m |2022-04-21T18:08:56.109183 |POINT (37.53507 55.81749) |\n|1 h 3 m |40 m |2 h 4 m  |59 m    |2022-04-21T18:09:12.827246 |POINT (37.54305 55.81759) |\n:::\n:::\n\n\nThe geometries are printed in the well-known text (WKT) format, but in the database they are stored in its binary version (WKB).\n\n## `shp2pgsql`\n\nThe tool for importing vector data is called `shp2pgsql`.\nAs the name suggests, it reads in an ESRI shapefile and generates SQL commands that load its data into a table.\nRunning the command without any arguments prints the usage information with description of all available options.\nI don't remember all the flags by heart, and refer to this help every time.\n\nBefore loading the data I usually create a descriptive schema to keep the database organized.\nFor example, in the project for which I collected this data, there are multiple tables for grids of different cities.\nIt makes sense to store all of such tables in the same schema.\n\n```sql\nCREATE SCHEMA times;\n```\n\nHere is the command I would use to load the example file into the newly created database:\n\n```sh\nshp2pgsql -s 4326 -c -I -S moscow_raw_subset.shp times.moscow | psql projects\n```\n\nIt specifies that the data uses WGS84 coordinate reference system (SRID 4326), that a new table should be created, that a spatial index should be created after the data is loaded, that simple geometries should be created instead of multi-geometries.\nThen comes the input file and the table name with the schema.\nThe tool generates SQL commands and prints them to standard output, and the easiest way to apply them is to pipe this output into `psql`.\n\nAfter running this command, a table with the data will be created in the `projects` database.\nThe shapefile is not necessary anymore, since the data is safe and easily accessible.\n\n# Uploading raster data\n\nIt's harder to imagine fitting raster data into a table.\nPostGIS offers a way to do that by storing binary chunks as rows.\nI didn't dive into details of the implementation and format, and it is not necessary for everyday use.\n\n## Example data\n\nAs an example raster I will use a digital terrain model from the NewFor individual tree detection dataset.\nHere is how it looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(terra)\nlibrary(tmap)\n\ntif <- here(\"posts\", \"2022-05-spatial-databases\", \"tif\")\ndtm_tif <- file.path(tif, \"15_leskova_dtm.tif\")\ndtm <- rast(dtm_tif)\n\ndtm |>\n  tm_shape() +\n  tm_raster(style = \"cont\", title = \"Elevation [m]\") +\n  tm_layout(legend.outside = TRUE)\n```\n\n::: {.cell-output-display}\n![An example raster. Digital terrain model from the NewFor individual tree detection benchmark.](index_files/figure-html/fig-example-raster-1.png){#fig-example-raster width=672}\n:::\n:::\n\n\n## `raster2pgsql`\n\nSimilarly to the vector data, the tool for importing rasters is called `raster2pgsql`.\nJust like it's vector counterpart, it reads in a raster file and generates SQL commands that load its data into a table.\nThis command has a lot more options, which are also printed with descriptions when the command is called without arguments.\n\nFor rasters I also usually create separate schemas.\nFor example, all digital terrain models will nicely fit into a `dtms` schema:\n\n```sql\nCREATE SCHEMA dtms;\n```\n\nHere is the command I would use to load the file into the `projects` database:\n\n```sh\nraster2pgsql -s 949910 15_leskova_dtm.tif dtms._15_leskova | psql projects\n```\n\nSimilarly, it specifies the SRID of the coordinate reference system of the raster.\nThen it provides the input file, table name with the schema, and pipes the SQL into `psql`.\n\nOne of the most useful arguments for this command is `-t WIDTHxHEIGH`.\nIt allows splitting the raster into chunks of fixed size, each stored on a separate row in the table.\nIn this format, rasters can be loaded partially, which is extremely useful for large datasets.\nI don't use the flag here because the example file is small enough to be loaded in one go, so can be stored in a single row.\n\n# Backing up the database\n\nOne of the advantages of using databases I listed in the introduction was the ease of backup.\nThe command below will create an SQL dump of the `projects` database.\n\n```sh\npg_dump -O projects -f projects_dump.sql\n```\n\nThe `-O` flag tells `pg_dump` to not record database user information.\nWithout it the databases will be assigned the same owner roles everywhere.\nI use it because on different machines the users are named differently, and there might be problems if there is a mismatch.\nThis creates an SQL dump: a file with SQL commands that recreate the database in its current state.\n\nTo restore from a backup, first an empty database needs to be created.\nThen `psql` can just execute the SQL dump file in this database, and everything will be restored.\n\n```sh\ncreatedb projects\npsql projects < projects_dump.sql\n```\n \n# Accessing the data\n\nNow that the data is safely stored in a database, the next question is how to access it.\nI usually only interact with my spatial data through either QGIS or R.\nQGIS has a very intuitive support for loading data from PostGIS databases right in the Browser window.\nIf there is a need to extract the data into files, QGIS provides a straight forward way to export the data into whatever format necessary.\nAnd I plan to dedicate a later post to interacting with the data from R.\n\nI'd also like to mention that PostGIS comes with a tool that allows exporting vector tables into ESRI shapefiles.\nIt does the reverse of what `shp2pgsql` does, and its appropriately named `pgsql2shp`.\nI have never used this tool, because if I need to extract data I do it through QGIS.\nBut for the sake of completeness, here is a command that would extract our example vector data back into a shapefile:\n\n```sh\npgsql2shp -f moscow_raw_subset.shp projects times.moscow\n```\n\n# Conclusion\n\nPostGIS is amazing. Highly recommend.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}