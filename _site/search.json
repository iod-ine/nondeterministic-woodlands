[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nondeterministic woodlands",
    "section": "",
    "text": "Stratified k-fold cross-validation in PySpark\n\n\n\n\n\n\n\nPython\n\n\nPySpark\n\n\n\n\nRunning stratified k-fold cross-validation in pure PySpark by splitting the data into folds manually and passing them to the foldCol argument.\n\n\n\n\n\n\nApr 17, 2023\n\n\nIvan Dubrovin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The who\nI’m Ivan, a PhD student at Skoltech. I research fusion of LiDAR with other remote sensing data for precision forestry.\n\n\nThe what\nMy journey through learning geospatial data science.\n\n\nThe how\nRMarkdown, Quarto, R, Python, RStudio, Netlify, giscus, and Google Domains: these were the ingredients chosen to create the perfect little blog."
  },
  {
    "objectID": "posts/2023-04-17-stratified-cv-pyspark/index.html",
    "href": "posts/2023-04-17-stratified-cv-pyspark/index.html",
    "title": "Stratified k-fold cross-validation in PySpark",
    "section": "",
    "text": "I have recently been experimenting with implementing automated feature extraction and evaluation techniques in PySpark. Some of them rely on running repeated k-fold cross-validation. PySpark provides the CrossValidator object for precisely that. However, simple random splits are not a suitable approach when the data is heavily imbalanced, which it often is. This short post shows a way to use the CrossValidator for running stratified k-fold cross-validation that keeps the class distribution similar across the folds.\nimport pyspark.sql.functions as F\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.sql.window import Window\nLet’s say we have completed all the preprocessing and stored the results in a Data Frame proc that has two columns: target and features. The target in this minimal example is binary, but the same approach applies to a multiclass case. For regression, an additional step of creating quantile buckets can be added to use this approach. The features column is the vector of features: output of the VectorAssembler transformer. Its schema would look like this:\n\nproc.printSchema()\n\nroot\n |-- target: long (nullable = true)\n |-- features: vector (nullable = true)\n\n\n\nThe data in proc is imbalanced, so pure k-fold cross-validation is unreliable. However, CrossValidator provides a possibility to specify the folds manually, which allows us to run more fancy versions of cross-validation using the same object. The folds have to be stored in a column whose name is passed to the foldCol argument of the CrossValidator constructor. During \\(i\\)th iteration, rows with value \\(i\\) in the foldCol are used as the validation set, and the rest are used for training the model.\nOne simple way to split the data into folds in a stratified fasion is to apply the ntile window function using partitions of the data by target and ordering by a random column to shuffle:\nk = 5\nproc = proc.withColumn(\n    \"fold\",\n    F.ntile(k).over(Window.partitionBy(\"target\").orderBy(F.rand())) - 1,\n)\nThe function assigns every row to one of \\(k\\) evenly split groups. Subtracting one converts the tile number to the index of the fold. Here is how the data is split across the folds:\n\nproc.groupBy(\"fold\", \"target\").count().show()\n\n                                                                                \n\n\n+----+------+------+\n|fold|target| count|\n+----+------+------+\n|   0|     0|234162|\n|   1|     0|234161|\n|   2|     0|234161|\n|   3|     0|234161|\n|   4|     0|234161|\n|   0|     1| 29987|\n|   1|     1| 29987|\n|   2|     1| 29987|\n|   3|     1| 29987|\n|   4|     1| 29987|\n+----+------+------+\n\n\n\n                                                                                \n\n\nAll folds have consistent target rates and shoud provide a more stable estimation of the error. Now set up a model and an evaluator:\nrfc = RandomForestClassifier(\n    featuresCol=\"features\",\n    labelCol=\"target\",\n    predictionCol=\"prediction\",\n    probabilityCol=\"probability\",\n    rawPredictionCol=\"rawPrediction\",\n    numTrees=350,\n    maxDepth=7,\n    maxBins=128,\n    minInstancesPerNode=5,\n)\nbinary_evaluator = BinaryClassificationEvaluator(\n    rawPredictionCol=\"rawPrediction\",\n    labelCol=\"target\",\n    metricName=\"areaUnderROC\",\n)\nAnd run the cross-validation:\ncv = CrossValidator(\n    estimator=rfc,\n    estimatorParamMaps=ParamGridBuilder().build(),\n    evaluator=binary_evaluator,\n    numFolds=k,\n    foldCol=\"fold\",\n)\ncvm = cv.fit(proc)\nAnd the result is:\n\ncvm.avgMetrics\n\n[0.6324294190273916]"
  }
]